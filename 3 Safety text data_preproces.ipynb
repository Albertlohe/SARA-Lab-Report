{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dc905e",
   "metadata": {},
   "source": [
    "\n",
    "# Safety Text Data Preprocessing Pipeline Demonstration\n",
    "\n",
    "This notebook demonstrates **text data preprocessing for safety-related text data** . The following steps will be demonstrated as a SOP for text data preprocessing:\\\n",
    "    1. Document collection  \n",
    "    2. Document standardization  \n",
    "    3. Tokenization  \n",
    "    4. Stopword and punctuation filtering  \n",
    "    5. Stemming and lemmatization  \n",
    "    6. Part-of-speech (POS) tagging  \n",
    "    7. Phrase recognition  \n",
    "    8. Named Entity Recognition (NER)  \n",
    "    9. Parsing  \n",
    "    10. Vector generation  \n",
    "    11. Feature generation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48817aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk #natural language toolkit\n",
    "\n",
    "nltk.download('punkt') #for sentence tokenizer\n",
    "nltk.download('stopwords') #for stop words\n",
    "nltk.download('wordnet')  #lexical database #corpus # synonyms #word relationships\n",
    "nltk.download('omw-1.4') #Open Multilingual Wordnet (1.4) #lemmatization #word base form\n",
    "nltk.download('averaged_perceptron_tagger') #pre-trained Part-of-Speech (POS) tagging model #initiates the download of the dataset and models necessary for the PerceptronTagger\n",
    "nltk.download('maxent_ne_chunker') #a pre-trained, Maximum Entropy-based model used for Named Entity Recognition (NER) to identify and classify entities (e.g., persons, organizations, locations) in text\n",
    "nltk.download('words') # lexical resource (\"words\") into your local NLTK data directory. This resource is a large corpus (collection of text or data) containing a standard English word list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08ca7c-8c03-490a-80be-2f461b37b2ea",
   "metadata": {},
   "source": [
    "## 1. Document Collection (Importing Text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "23b845c4-3656-4c1d-b960-a6a7831f1b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Narratives</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Narrative-4</td>\n",
       "      <td>A car moving on the left lane of the road is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Narratives                                        description\n",
       "0  Narrative-4  A car moving on the left lane of the road is t..."
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Text data from Excel/CSV file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_excel('C:/Users/hp/Desktop/SARALab_coding/3_unstruc_data_preproc/safety_narrative_1.xlsx')\n",
    "df = pd.read_excel('safety_narrative_1.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7656f0",
   "metadata": {},
   "source": [
    "## 2. Document Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2891c124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A car moving on the left lane of the road is t...</td>\n",
       "      <td>a car moving on the left lane of the road is t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  A car moving on the left lane of the road is t...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  a car moving on the left lane of the road is t...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardizing safety narrative data; e.g lower capitals for all words\n",
    "\n",
    "def standardize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['description'].apply(standardize_text) #adding column 'clean_text'\n",
    "df[['description', 'clean_text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45323fd8",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522f4497",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m].apply(word_tokenize)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#Output\u001b[39;00m\n\u001b[32m      8\u001b[39m df[[\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m,\t\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "\n",
    "#Output\n",
    "df[['description',\t'clean_text', 'tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247cbf6",
   "metadata": {},
   "source": [
    "## 4. Stopword and Punctuation Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "14dfc9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A car moving on the left lane of the road is t...</td>\n",
       "      <td>a car moving on the left lane of the road is t...</td>\n",
       "      <td>[a, car, moving, on, the, left, lane, of, the,...</td>\n",
       "      <td>[car, moving, left, lane, road, taking, right,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  A car moving on the left lane of the road is t...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  a car moving on the left lane of the road is t...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [a, car, moving, on, the, left, lane, of, the,...   \n",
       "\n",
       "                                 tokens_no_stopwords  \n",
       "0  [car, moving, left, lane, road, taking, right,...  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For stop words and punctuation\n",
    "# Stop words mean insignificant words which carry no meaning; remove punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['tokens_no_stopwords'] = df['tokens'].apply(\n",
    "    lambda x: [word for word in x if word not in stop_words]\n",
    ")\n",
    "\n",
    "#Output\n",
    "\n",
    "df[['description',\t'clean_text', 'tokens','tokens_no_stopwords']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3d3f2",
   "metadata": {},
   "source": [
    "## 5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3ff439a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[car, moving, left, lane, road, taking, right,...</td>\n",
       "      <td>[car, move, left, lane, road, take, right, tur...</td>\n",
       "      <td>[car, moving, left, lane, road, taking, right,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tokens_no_stopwords  \\\n",
       "0  [car, moving, left, lane, road, taking, right,...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [car, move, left, lane, road, take, right, tur...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [car, moving, left, lane, road, taking, right,...  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming and Lemmatization\n",
    "\n",
    "#Stemming a process for reducing inflected words to their root or base form (the \"stem\")..\n",
    "# ..by removing affixes, primarily suffixes\n",
    "\n",
    "# Lemmatization reduces words to their base or dictionary form (lemma) based on their meaning and part of speech (POS). \n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['stemmed'] = df['tokens_no_stopwords'].apply(\n",
    "    lambda x: [stemmer.stem(word) for word in x]\n",
    ")\n",
    "\n",
    "df['lemmatized'] = df['tokens_no_stopwords'].apply(\n",
    "    lambda x: [lemmatizer.lemmatize(word) for word in x] #  reduce a list of words (tokens) to their base or dictionary form (lemmas). \n",
    ")\n",
    "\n",
    " #To get accurate results for verbs or adjectives\n",
    "\n",
    "#lambda x: [lemmatizer.lemmatize(word, pos = 'v') for word in x]\n",
    "\n",
    "#Output\n",
    "df[['tokens_no_stopwords', 'stemmed', 'lemmatized']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f816ba",
   "metadata": {},
   "source": [
    "## 6. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "727b21eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[car, moving, left, lane, road, taking, right,...</td>\n",
       "      <td>[(car, NN), (moving, VBG), (left, VBD), (lane,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          lemmatized  \\\n",
       "0  [car, moving, left, lane, road, taking, right,...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(car, NN), (moving, VBG), (left, VBD), (lane,...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of speech (pos) \n",
    "#The process of assigning grammatical categories (like noun, verb, adjective) to words in a text\n",
    "\n",
    "df['pos_tags'] = df['lemmatized'].apply(nltk.pos_tag)\n",
    "df[['lemmatized', 'pos_tags']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "568a8e2a-536e-4f75-af06-84fd4970f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file named 'output.csv'\n",
    "df.to_csv('Example1_output.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aab03c",
   "metadata": {},
   "source": [
    "## 7. Phrase Recognition (Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5f13179b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lane', 'road'),\n",
       " ('apply', 'brake'),\n",
       " ('avoid', 'collision'),\n",
       " ('balance', 'fall'),\n",
       " ('brake', 'avoid'),\n",
       " ('car', 'moving'),\n",
       " ('coming', 'high'),\n",
       " ('high', 'speed'),\n",
       " ('indicator', 'time'),\n",
       " ('lose', 'balance')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram a bigram is a sequence of two adjacent words (or tokens) from a text \n",
    "# generated using functions like nltk.bigrams() to help analyze word co-occurrence.\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "all_tokens = [word for tokens in df['lemmatized'] for word in tokens]\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "bigrams = bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "\n",
    "#Output\n",
    "\n",
    "bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924486bf",
   "metadata": {},
   "source": [
    "## 8. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f9d8f01-06d0-41b3-b77c-c74b584c6b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "df3276c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>named_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[car, moving, left, lane, road, taking, right,...</td>\n",
       "      <td>[(car, NN), (moving, VBG), (left, VBD), (lane,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          lemmatized  \\\n",
       "0  [car, moving, left, lane, road, taking, right,...   \n",
       "\n",
       "                                      named_entities  \n",
       "0  [(car, NN), (moving, VBG), (left, VBD), (lane,...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER identifies and classifies key entities in unstructured text, such as people, organizations, locations, dates, and quantities.\n",
    "# NER transforms raw text into structured data for better machine understanding\n",
    "def get_named_entities(tokens):\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    chunks = nltk.ne_chunk(pos_tags)\n",
    "    return chunks\n",
    "\n",
    "    \n",
    "#Output\n",
    "df['named_entities'] = df['lemmatized'].apply(get_named_entities)\n",
    "df[['lemmatized', 'named_entities']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca2365",
   "metadata": {},
   "source": [
    "## 9. Parsing (Shallow Parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c1cf903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>parsed_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(car, NN), (moving, VBG), (left, VBD), (lane,...</td>\n",
       "      <td>[[(car, NN)], (moving, VBG), (left, VBD), [(la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pos_tags  \\\n",
       "0  [(car, NN), (moving, VBG), (left, VBD), (lane,...   \n",
       "\n",
       "                                      parsed_phrases  \n",
       "0  [[(car, NN)], (moving, VBG), (left, VBD), [(la...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The process of analyzing a raw sequence of text (tokens) to determine its grammatical structure and the relationships between words.\n",
    "# Converts unstructured information into a structured, machine-readable format\n",
    "\n",
    "\n",
    "grammar = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "#Output\n",
    "df['parsed_phrases'] = df['pos_tags'].apply(cp.parse)\n",
    "df[['pos_tags', 'parsed_phrases']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cca2c7",
   "metadata": {},
   "source": [
    "## 10. Vector Generation (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cbd426-ca86-4632-957c-c6828bca2c98",
   "metadata": {},
   "source": [
    "### Example of TF-IDF vector generation using sklearn library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "efbbe9f7-d9ad-4245-867f-5ca3b2c814e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Let's say we have three sample documents\n",
    "documents = [\"I love playing guitar\", \"I love singing\", \"playing guitar is fun\"]\n",
    "# type(documents)  #list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a16d528-878e-4897-ae5f-68348d2ebc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #for excluding stop words\n",
    "# vectorizer\n",
    "\n",
    "matrix = vectorizer.fit_transform(documents) ## text to number #L2 transformation\n",
    "\n",
    "\n",
    "# matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a19dca3d-e0a6-4bb4-82ab-1a75a320fad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fun', 'guitar', 'love', 'playing', 'singing'], dtype=object)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the all the features\n",
    "\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b30fe19-48b0-482a-99d0-41ffa04eaea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 2, 'playing': 3, 'guitar': 1, 'singing': 4, 'fun': 0}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the word index\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16811a5c-4ec0-4d9c-8fb3-ad9bb68e913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.57735027 0.57735027 0.57735027 0.        ]\n",
      " [0.         0.         0.60534851 0.         0.79596054]\n",
      " [0.68091856 0.51785612 0.         0.51785612 0.        ]]\n",
      "['fun' 'guitar' 'love' 'playing' 'singing']\n"
     ]
    }
   ],
   "source": [
    "# Convert to dense format to view the tf-idf vector\n",
    "print(matrix.todense())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c020f02-c03f-4197-8952-9f05863f41b7",
   "metadata": {},
   "source": [
    "### *Feature selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c279b4db-5157-428e-b2b5-78e2735c9e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'love' 'singing']\n"
     ]
    }
   ],
   "source": [
    "#Feature selection\n",
    "import numpy as np\n",
    "\n",
    "X = matrix.toarray()   # convert sparse → dense\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# max TF-IDF value of each feature across documents\n",
    "max_tfidf = X.max(axis=0)\n",
    "\n",
    "# select features with max TF-IDF ≥ 0.6\n",
    "selected_features = feature_names[max_tfidf >= 0.6]\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd4e55-c1b9-4fa6-b248-6ef860bb2bd5",
   "metadata": {},
   "source": [
    "### *Continue with the example document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f7afe7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>apply</th>\n",
       "      <th>at</th>\n",
       "      <th>avoid</th>\n",
       "      <th>balance</th>\n",
       "      <th>bike</th>\n",
       "      <th>biker</th>\n",
       "      <th>brake</th>\n",
       "      <th>can</th>\n",
       "      <th>car</th>\n",
       "      <th>...</th>\n",
       "      <th>same</th>\n",
       "      <th>speed</th>\n",
       "      <th>suddenly</th>\n",
       "      <th>suffer</th>\n",
       "      <th>taking</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "      <th>to</th>\n",
       "      <th>turn</th>\n",
       "      <th>without</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     and   apply      at   avoid  balance   bike  biker   brake     can  \\\n",
       "0  0.125  0.0625  0.1875  0.0625   0.0625  0.125  0.125  0.0625  0.0625   \n",
       "\n",
       "      car  ...    same   speed  suddenly  suffer  taking     the    time  \\\n",
       "0  0.0625  ...  0.0625  0.0625    0.0625  0.0625  0.0625  0.8125  0.0625   \n",
       "\n",
       "       to    turn  without  \n",
       "0  0.0625  0.0625   0.0625  \n",
       "\n",
       "[1 rows x 42 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF: Term frequency\n",
    "# IDF: Inverse Document Frequency)\n",
    "\n",
    "# A fundamental technique in NLP that transforms raw, unstructured text data into meaningful numerical vectors\n",
    "# TF-IDF highlights important words in a document while reducing the weight of commonly occurring words across a corpus. \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(df['clean_text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Vector tf-idf matrix\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "11c571ff-296b-4de4-8eaf-3dd63a5c3734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a car moving on the left lane of the road is t...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score for rare terms is highher\n",
    "# Score for frequent terms is lower.\n",
    "\n",
    "# df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "338ab02d-b390-445b-9a84-c1b4b93b8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the word index\n",
    "\n",
    "# tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb3fa2a6-0e53-408c-a810-41b358f65f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'apply', 'at', 'avoid', 'balance', 'bike', 'biker', 'brake',\n",
       "       'can', 'car', 'collision', 'coming', 'crossing', 'fall', 'from',\n",
       "       'he', 'high', 'his', 'indicator', 'injuries', 'is', 'lane', 'left',\n",
       "       'lose', 'major', 'may', 'moving', 'of', 'off', 'on', 'right',\n",
       "       'road', 'same', 'speed', 'suddenly', 'suffer', 'taking', 'the',\n",
       "       'time', 'to', 'turn', 'without'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check all features\n",
    "\n",
    "# tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0fd3623-4027-4555-8df0-3c238c13f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "\n",
    "tfidf_df.to_csv('Example1_vector.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd7361ea-f1ed-49c6-a41b-a7ba31169caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving features in array\n",
    "\n",
    "# tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe508d",
   "metadata": {},
   "source": [
    "## 11. Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "62a52365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A car moving on the left lane of the road is t...</td>\n",
       "      <td>71</td>\n",
       "      <td>43</td>\n",
       "      <td>3.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  word_count  \\\n",
       "0  A car moving on the left lane of the road is t...          71   \n",
       "\n",
       "   unique_word_count  avg_word_length  \n",
       "0                 43         3.929577  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature generation \n",
    "\n",
    "df['word_count'] = df['tokens'].apply(len)\n",
    "df['unique_word_count'] = df['tokens'].apply(lambda x: len(set(x)))\n",
    "df['avg_word_length'] = df['clean_text'].apply(\n",
    "    lambda x: sum(len(word) for word in x.split()) / len(x.split())\n",
    ")\n",
    "\n",
    "df[['description', 'word_count', 'unique_word_count', 'avg_word_length']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b0789b3d-c8e4-4cb5-aeb7-aedfca200d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>at</th>\n",
       "      <th>bike</th>\n",
       "      <th>biker</th>\n",
       "      <th>crossing</th>\n",
       "      <th>is</th>\n",
       "      <th>lane</th>\n",
       "      <th>may</th>\n",
       "      <th>of</th>\n",
       "      <th>on</th>\n",
       "      <th>right</th>\n",
       "      <th>road</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     and      at   bike  biker  crossing     is   lane    may     of      on  \\\n",
       "0  0.125  0.1875  0.125  0.125     0.125  0.125  0.125  0.125  0.125  0.1875   \n",
       "\n",
       "   right   road     the  \n",
       "0  0.125  0.125  0.8125  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum TF-IDF value of each feature across all documents\n",
    "max_tfidf = tfidf_df.max(axis=0)\n",
    "\n",
    "# Select features with TF-IDF >= 0.1\n",
    "selected_features = max_tfidf[max_tfidf >= 0.1].index\n",
    "\n",
    "# Reduced TF-IDF dataframe\n",
    "tfidf_selected = tfidf_df[selected_features]\n",
    "\n",
    "tfidf_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b7e2392c-6072-4144-917c-e692293e7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file \n",
    "\n",
    "tfidf_selected.to_csv('Features_final.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1eed4-7b94-4ef1-9959-424d14351a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9eb476c-4eb9-46b9-8be7-e9db5d7b5281",
   "metadata": {},
   "source": [
    "### Next Model building (Do-it-Yourself)\n",
    "   ### *Each selected feature becomes a predictor variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75c9e1-5f31-4cd0-a74d-f6d2aabb9445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc331b2-d20d-4163-835a-c2c069097fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e301ce-4568-4ca1-8db4-2666d6ba122b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50da12d2",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This code demonstrates a **complete text data analytics preprocessing pipeline**\n",
    "for **safety-related text data**, which can be directly extended to modelling such as:\n",
    "- Topic modeling (LDA / NMF)\n",
    "- Accident severity classification\n",
    "- Safety risk analysis\n",
    "- NLP-based safety analytics\n",
    "- Document cluster based on the selected features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897cd14-9351-48f0-bebf-6e3477f36839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dd5a459-0fff-4c70-bfd1-6c8be951041e",
   "metadata": {},
   "source": [
    "# Problem statement and to do\n",
    "\n",
    "1. For the given safety text data in the Excel spreadsheet shared on **Moodle**, apply all the above-discussed text data preprocessing steps.\n",
    "2. Develop a **tf-idf** vector matrix.\n",
    "3. Select important features and decide on your **own decision rule**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983e521-361b-4f0d-9a27-fa5bf2b820aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
